{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOMATIC DIFFERENTIATION WITH TORCH.AUTOGRAD\n",
    "When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters are adjusted according to the gradient of loss function with respect to given parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True) # weight\n",
    "b = torch.randn(3, requires_grad=True) # bias\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors, Functions and computational graph\n",
    "![](https://pytorch.org/tutorials/_images/comp-graph.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Function for z =  <AddBackward0 object at 0x7fcb95fece50>\n",
      "Gradient function for loss =  <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fcb95fec430>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient Function for z = ', z.grad_fn)\n",
    "print('Gradient function for loss = ', loss.grad_fn)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Gradient\n",
    "To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely we need $\\frac{\\partial loss}{\\partial a}$ and $\\frac{\\partial loss}{\\partial b}$ undersome fixed values of x and y. To compute those derivatives, we call `loss.backward()` and then retrieve the values from `w.grad` and `b.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0326, 0.1933, 0.1209],\n",
      "        [0.0326, 0.1933, 0.1209],\n",
      "        [0.0326, 0.1933, 0.1209],\n",
      "        [0.0326, 0.1933, 0.1209],\n",
      "        [0.0326, 0.1933, 0.1209]])\n",
      "tensor([0.0326, 0.1933, 0.1209])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disabling gradient tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x,w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "z = (torch.matmul(x,w) + b).detach()\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasons to disable gradient tracking:\n",
    "- To mark some parameters in the network as frozen parameters.\n",
    "- To speed up computations when only doing the forward pass, because computation gradient on tensors that do not track gradients would be more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Gradient and Jacobian Product\n",
    "\n",
    "In many cases, there are scalar loss function, and we need to compute the gradient with respect to some parameters. However there are cases when the output function is arbitrary tensor. In this case pytorch allow to compute jacobian product and not actial gradient.\n",
    "\n",
    "\n",
    "\n",
    "For a vector function  $\\vec{y}=f(\\vec{x})$, where\n",
    "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ and\n",
    "$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$, a gradient of\n",
    "$\\vec{y}$ with respect to $\\vec{x}$ is given by **Jacobian\n",
    "matrix**:\n",
    "$J=\\left(\\begin{array}{ccc}\n",
    "       \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "       \\vdots & \\ddots & \\vdots\\\\\n",
    "       \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "       \\end{array}\\right)$\n",
    "Instead of computing the Jacobian matrix itself, PyTorch allows you to computer Jacobian Product $v^T\\cdot J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Call\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "\n",
      "Second call\n",
      " tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp+1).pow(2)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"First Call\\n\", inp.grad)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nSecond call\\n\", inp.grad)\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
